{
    "Title": "COVID-19 Transformer Model (SciBERT)",
    "Tags": ["Natural Language Processing"],
    "Architecture": "BERT",
    "Publisher": [["Tanmay Thakur","https://github.com/lordtt13"], ["Smoketrees","https://github.com/smoke-trees"], ["CYBINT","https://github.com/CYBINT-IN"]],
    "Problem Domain": "Text",
    "Model Format": "Transformer",
    "Language": "English",
    "Dataset": [["CORD-19","https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge"]],
    "Overview": "A BERT model trained on the unsupervised data present in the COVID research papers with a SciBERT base.",
    "Preprocessing": "preprocessing.html",
    "Link": "https://drive.google.com/file/d/1idk98WbPMOAwQlqDCLSl6fdlQdTR2JaD/view?usp=sharing",
    "Usage": "usage.html",
    "References": ["https://arxiv.org/abs/1810.04805"],
    "Input Shape": [["Tokenize using tokenizer"]],
    "Output Shape": [["Transformer decides"]],
    "Description": "BERT (Bidirectional Encoder Representations from Transformers)\nBERT applies the bidirectional training of Transformer, to language modelling.\nThis is in contrast to previous efforts which looked at a text sequence either\nfrom left to right or combined left-to-right and right-to-left training\nBidirectional Training makes the model have a better sense of language than\nuni-directional language models.\nBERT makes use of Transformer which is an attention mechanism that learns contextual relations between words.\nTransformer includes two separate mechanisms — an encoder that reads the text input and a decoder that produces\na prediction for the task. Since BERT’s goal is to generate a language model, only the encoder mechanism is necessary.\nBERT can be used for a wide variety of language tasks with little tweaks,\nThis model accompolishes classification tasks such as sentiment analysis are\ndone similarly to Next Sentence classification, by adding a classification layer on top of the Transformer output.\n\nSciBERT is a Pretrained Language Model for Scientific Text.SciBERT leverages unsupervised pretraining on a large multi-domain\ncorpus of scientific publications to improve performance on downstream scientific NLP tasks.\n We enhanced the CORD vocabulary and made it into an MLM(Masked Language Model) to get the results.\nBefore feeding word sequences into BERT, a fixed percentage of the words in each sequence are replaced with a [MASK] token. The model attempts to predict the original value of the masked words, based on the context provided by the other, non-masked, words in the sequence."
}
